{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from math import cos, pi\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data    \n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import itertools\n",
    "import io\n",
    "from imblearn.over_sampling import SMOTE\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device='cpu'\n",
    "import read_preprocess\n",
    "from models import DNN, dataset_csv\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f76bcac8790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/user/Sarvesh/Baap_Product/AutoML_DL/Cement/train_concrete.csv')\n",
    "sns.barplot(data=df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28     342\n",
       "3      113\n",
       "7      104\n",
       "56      66\n",
       "14      48\n",
       "90      44\n",
       "100     40\n",
       "91      20\n",
       "180     19\n",
       "270     13\n",
       "365     10\n",
       "360      2\n",
       "1        2\n",
       "120      1\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretize(BaseEstimator, TransformerMixin):\n",
    "    def fit(self,X:pd.DataFrame):\n",
    "        self.X = X\n",
    "        self.X_num = self.X.select_dtypes(exclude=['category','uint8'])\n",
    "        self.cols = self.X_num.columns.tolist()\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        df_disc = pd.DataFrame()\n",
    "        for col in self.cols:\n",
    "            df_disc[col] = pd.cut(self.X_num[col],5,labels=['0_20','20_40','40_60','60_80','80_100'])\n",
    "        df_disc = pd.get_dummies(df_disc,drop_first=False)\n",
    "        \n",
    "        return pd.concat([self.X,df_disc],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13317\n",
       "2     4300\n",
       "3     3256\n",
       "4     1910\n",
       "5     1337\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.read_csv('./adv_train.csv', index_col=False)\n",
    "target = \"Rating\"\n",
    "frame['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Frames: 0\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sparse(frame_sparse):\n",
    "    bad_na, good_na = [],[]\n",
    "    for column in frame_sparse:\n",
    "        if frame_sparse[column].isna().sum()/frame_sparse.shape[0] > 0.3:\n",
    "            frame_sparse.append(column)\n",
    "        elif frame_sparse[column].isna().sum()/frame_sparse.shape[0] <= 0.3 and frame_sparse[column].isna().any()==True:\n",
    "            good_na.append(column)\n",
    "        frame_sparse = frame_sparse.drop(bad_na,axis=1)\n",
    "    print(f\"Bad Frames: {len(bad_na)}\")\n",
    "    for col in good_na:\n",
    "        frame_sparse[col] = frame_sparse[col].fillna(stats.stats.mode(frame_sparse[col])[0][0])\n",
    "    return frame_sparse\n",
    "frame = preprocess_sparse(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = './glove.twitter.27B/glove.twitter.27B.25d.txt'\n",
    "def load_glove(path):\n",
    "    \"\"\"\n",
    "    creates a dictionary mapping words to vectors from a file in glove format.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        glove = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove[word] = vector\n",
    "        return glove\n",
    "    \n",
    "def load_glove_embeddings(path, word2idx, embedding_dim=25):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            index = word2idx.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.replace('<br />',' ')\n",
    "    text = text.replace('\\x96',' ')\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [w.lower() for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = Discretize().fit_transform(frame[['Age']]).drop(['Age'],axis=1)\n",
    "frame = frame.loc[:,'Rating':'unfav']\n",
    "frame_sparse = frame.loc[:,'Rating':'Mostwatchedtvprogrammes_Weather']\n",
    "frame_sparse = pd.concat((frame_sparse,age), axis=1)\n",
    "\n",
    "frame_x_text = frame.loc[:,'fav':'unfav']\n",
    "\n",
    "x_train_text = []\n",
    "for i in frame_x_text['fav']+frame_x_text['unfav']:\n",
    "    text = tokenizer(i)\n",
    "    x_train_text.append(text)\n",
    "    \n",
    "# x_val_text = []\n",
    "# for i in X_val_text_df['fav']+X_val_text_df['unfav']:\n",
    "#     text = tokenizer(i)\n",
    "#     x_val_text.append(text)\n",
    "    \n",
    "all_tokens = itertools.chain.from_iterable(x_train_text)\n",
    "word_to_id = {token:idx for idx, token in enumerate(set(all_tokens))}\n",
    "\n",
    "all_tokens = itertools.chain.from_iterable(x_train_text)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)\n",
    "\n",
    "\"\"\" Load Train Embeddings \"\"\"\n",
    "glove = load_glove(glove_path)\n",
    "train_embeddings = load_glove_embeddings(glove_path, word_to_id)\n",
    "\n",
    "max_len = max([len(i) for i in x_train_text])\n",
    "x_train_tokenized_sentences = np.array([[word_to_id[i] for i in j] for j in x_train_text])\n",
    "# x_val_tokenized_sentences = np.array([[word_to_id[i] for i in j] for j in x_val_text])\n",
    "\n",
    "# # Pad tokens with 0s\n",
    "for i in range(len(x_train_tokenized_sentences)):\n",
    "    a = x_train_tokenized_sentences[i]\n",
    "    x_train_tokenized_sentences[i] = np.concatenate([a,np.zeros(max_len-len(a))])\n",
    "    \n",
    "# for i in range(len(x_val_tokenized_sentences)):\n",
    "#     a = x_val_tokenized_sentences[i]\n",
    "#     x_val_tokenized_sentences[i] = np.concatenate([a,np.zeros(max_len-len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1957.,  728., 1624.,  963.,  384., 1192.,  170.,  974.,    0.,\n",
       "        853., 1061.,  302.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "       array([ 803., 1946.,  234., 1946., 1745., 1946.,  750.,  275., 1946.,\n",
       "        452., 1946., 1526., 1827., 1946., 1615., 1946., 1333.,  209.,\n",
       "       1946.,  744., 1946.,  871., 1946.,  623.,  803., 1946.,  952.,\n",
       "       1946., 1693., 1946.,  885., 1946.,  799., 1946., 1772., 1946.,\n",
       "        266., 1209., 1946., 1245., 1946.,  946., 1641., 1946., 1295.,\n",
       "       1946.,  527., 1262., 1639., 1946., 1275., 1946.,  799., 1946.,\n",
       "        944.,  428., 1946., 1956., 1946., 1295., 1946.,  974.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "       array([ 747., 1946., 1263.,  972., 1946.,  955., 1210., 1946., 1426.,\n",
       "       1057., 1946., 1821.,  784., 1946.,  855., 1946.,    7., 1946.,\n",
       "        613., 1735., 1946.,  297., 1863., 1946., 1418.,   59., 1946.,\n",
       "        542., 1319., 1946.,  742.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "       ...,\n",
       "       array([1705.,  566., 1591.,  497., 1279., 1946.,  440.,  371., 1946.,\n",
       "         89., 1946.,  176., 1686., 1444., 1908.,  434.,  466., 1946.,\n",
       "       1873.,   54., 1946.,   89.,  866., 1686., 1815., 1946., 1001.,\n",
       "       1946., 1032., 1783., 1561., 1946.,   81., 1946., 1111.,  198.,\n",
       "        619., 1686., 1039., 1946., 1577., 1946.,  410., 1853., 1758.,\n",
       "        327.,  518., 1276., 1946.,  265., 1239., 1769.,  801.,  970.,\n",
       "        719.,  897., 1946., 1056., 1946., 1910., 1354.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "       array([1248.,  306., 1071., 1421.,  767., 1267.,  206., 1217.,  378.,\n",
       "        990.,  803.,  893., 1025.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.]),\n",
       "       array([1042.,  580., 1811., 1920., 1751.,  890., 1330.,  714.,  617.,\n",
       "        675., 1294.,  645., 1501.,  693.,  855., 1115.,  330., 1333.,\n",
       "        617., 1437.,  785., 1646., 1147., 1518.,  666., 1757.,  600.,\n",
       "       1935.,  834.,  499.,  802.,  875.,  462., 1732., 1211., 1036.,\n",
       "        123., 1935.,  107.,  391.,   94., 1068., 1037., 1838., 1305.,\n",
       "       1568.,  125., 1301., 1963.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.])], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_x_embedded = pd.DataFrame(x_train_tokenized_sentences.tolist())\n",
    "frame_x_embedded.columns = [str(i) for i in frame_x_embedded.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pd.concat((frame_sparse,frame_x_embedded), axis=1)\n",
    "combined_x = combined_dataset.loc[:,combined_dataset.columns != target]\n",
    "combined_y = pd.DataFrame(combined_dataset[target],columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_oversample = SMOTE()\n",
    "combined_x, combined_y = smote_oversample.fit_resample(combined_x, combined_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data = pd.concat((combined_x, combined_y),axis=1)\n",
    "# combined_data = combined_data.sample(frac=1)\n",
    "frame_x_sparse = combined_data.loc[:,'Gender_F':'Age_80_100']\n",
    "frame_x_embedded = combined_data.loc[:,'0':'105']\n",
    "frame_y = combined_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc = combined_data.iloc[:,-107:-1]\n",
    "cc = cc.reset_index(drop=True)\n",
    "rows = {\"0\":[]}\n",
    "for i in range(cc.shape[0]):\n",
    "    temp = cc.loc[i,:]\n",
    "    rows[\"0\"].append(temp.values.tolist())\n",
    "frame_x_embedded = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_x_embedded.columns=[\"fav_unfav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sparse = pd.concat((frame_x_sparse,frame_x_embedded),axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(frame_sparse.loc[:, frame_sparse.columns != target], frame_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning:\n",
      "\n",
      "The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "\n",
      "/home/user/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning:\n",
      "\n",
      "The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_stats = X_train.loc[:,'Gender_F':'Age_80_100']\n",
    "X_train_stats.to_csv('./processed_splits/X_train_stats.csv', index=False)\n",
    "X_train_text_df = X_train.loc[:,'fav_unfav']\n",
    "X_train_text_df.to_csv('./processed_splits/X_train_text.csv', index=False)\n",
    "X_val_stats = X_val.loc[:,'Gender_F':'Age_80_100']\n",
    "X_val_stats.to_csv('./processed_splits/X_val_stats.csv', index=False)\n",
    "X_val_text_df  = X_val.loc[:,'fav_unfav']\n",
    "X_val_text_df.to_csv('./processed_splits/X_val_text.csv', index=False)\n",
    "y_train_df = pd.DataFrame(y_train, columns=[target])-1 # Subtract one cos rating 1 to 5 instead of 0 to 4 \n",
    "y_train_df.to_csv('./processed_splits/y_train.csv', index=False)\n",
    "y_val_df  = pd.DataFrame(y_val, columns=[target])-1\n",
    "y_val_df.to_csv('./processed_splits/y_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53268, 263), (53268,), (53268,))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stats.shape, y_train.shape, X_train_text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13317,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_tokenized_sentences = X_train_text_df.values\n",
    "for i in range(len(x_train_tokenized_sentences)):\n",
    "    x_train_tokenized_sentences[i] = np.array(x_train_tokenized_sentences[i])\n",
    "x_val_tokenized_sentences = X_val_text_df.values\n",
    "for i in range(len(x_val_tokenized_sentences)):\n",
    "    x_val_tokenized_sentences[i] = np.array(x_val_tokenized_sentences[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sparse_embeddings = nn.Embedding(263, 64)\n",
    "sparse_embeddings(torch.tensor(X_train_stats.values[:64,:])).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding(torch.LongTensor([0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding = nn.Embedding.from_pretrained(train_embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input = torch.LongTensor([x_train_tokenized_sentences[0]])\n",
    "embedding(input).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(Dataset):\n",
    "    def __init__(self, X_stats, x_train_tokenized_sentences, y, target):\n",
    "        self.x1 = X_stats.values\n",
    "        self.x2 = x_train_tokenized_sentences\n",
    "        self.y = y[target].values\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.x1[idx, :]\n",
    "        x2 = self.x2[idx]\n",
    "        y = self.y[idx]\n",
    "        return x1, x2, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, train_embeddings, dropout,dim):\n",
    "        super().__init__()\n",
    "        dim = int(dim)\n",
    "        self.sparse_embeddings = nn.Embedding(263, 25)\n",
    "        self.text_embeddings = nn.Embedding.from_pretrained(train_embeddings)\n",
    "        self.emb_lin = nn.Linear(9225, dim*2)\n",
    "        self.lin1 = nn.Linear(int(dim*2), int(dim/2)) # 64 = glove embedding(25) + sparse embedding(39)\n",
    "        self.lin2 = nn.Linear(int(dim/2), int(dim/4))\n",
    "        self.lin3 = nn.Linear(int(dim/4), 5)\n",
    "#         self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "#         self.bn2 = nn.BatchNorm1d(200)\n",
    "#         self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x_sparse, x_text):\n",
    "        x = self.sparse_embeddings(x_sparse)\n",
    "        x2 = self.text_embeddings(x_text)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = x.view(-1, 9225)\n",
    "        x = self.emb_lin(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.logsoftmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cosine_lr(lr, epoch, step_size):\n",
    "    lr = lr/2 * (cos(pi*(epoch%step_size)/step_size)+1)\n",
    "    return lr\n",
    "\n",
    "def train(trial):\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size',[64,128,256,512])\n",
    "    params = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 250)\n",
    "#     step_size = trial.suggest_int(\"step_size\",5,100)\n",
    "    highest_acc = 0\n",
    "    dim = trial.suggest_categorical(\"layer_dims\", [32,64,128,256,512])\n",
    "    dropout = trial.suggest_uniform(\"dropout\",0.1,0.6)\n",
    "    net = Classifier(train_embeddings, dropout=dropout, dim=dim)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    training_set = DataReader(X_train_stats, x_train_tokenized_sentences, y_train_df, target)\n",
    "    train_loader = DataLoader(training_set, **params)\n",
    "    validation_set = DataReader(X_val_stats, x_val_tokenized_sentences, y_val_df, target)\n",
    "    val_loader = DataLoader(validation_set, **params)\n",
    "\n",
    "    net = net.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "#     low_lr = trial.suggest_loguniform(\"low_lr\", 1e-8,1e-4)\n",
    "#     cyclic_bool = trial.suggest_categorical(\"cyclic_bool\",[True, False])\n",
    "    plateau = 0\n",
    "    for epoch in range(epochs):\n",
    "        logs = {}\n",
    "        if False:\n",
    "            for a in optimizer.param_groups:\n",
    "                lr = cosine_lr(lr, epoch, step_size) + low_lr\n",
    "                a['lr'] = lr\n",
    "\n",
    "        t_loss, a, acc = 0, 0, 0\n",
    "        for x_sparse, x_text, y_train in train_loader:\n",
    "            a+=1\n",
    "\n",
    "            x_sparse, x_text, y_train = x_sparse.to(device, dtype = torch.int64), x_text.to(device, dtype = torch.int64), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            net.train()\n",
    "            y_pred = net(x_sparse, x_text)\n",
    "            train_loss = F.cross_entropy(y_pred,y_train)\n",
    "            t_loss += train_loss\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            acc += (y_pred.argmax(dim=1) == y_train).sum().float() / float(y_train.size(0))\n",
    "\n",
    "#         print(acc/a)\n",
    "        t_loss /= a\n",
    "        if epoch%1 == 0:\n",
    "            logs['CE Loss'] = t_loss / a\n",
    "            logs['Accuracy'] = acc/a\n",
    "            with torch.set_grad_enabled(False):\n",
    "                test_loss, a, acc = 0, 0, 0\n",
    "                for x_sparse_val, x_text_val, y_val in val_loader:\n",
    "                    a += 1\n",
    "                    x_sparse_val, x_text_val, y_val = x_sparse_val.to(device, dtype = torch.int64), x_text_val.to(device, dtype = torch.int64), y_val.to(device)\n",
    "                    y_test_pred = net(x_sparse_val, x_text_val)\n",
    "                    y_test_pred = torch.squeeze(y_test_pred)\n",
    "                    test_loss += F.cross_entropy(y_test_pred,y_val)\n",
    "                    acc += (y_test_pred.argmax(dim=1) == y_val).sum().float() / float(y_val.size(0))\n",
    "            if (acc/a)>highest_acc: \n",
    "                plateau = 0\n",
    "                highest_acc = acc/a\n",
    "#                 torch.save(net.state_dict(),\"./model_info/3_June_0.pth\")\n",
    "            else:\n",
    "                plateau += 1\n",
    "#             print(acc/a)\n",
    "            test_loss /= a\n",
    "\n",
    "#             logs['Val CE Loss'] = test_loss / a\n",
    "#             logs['Val Accuracy'] = acc/a\n",
    "#             logs['Highest Acc'] = highest_acc\n",
    "#             print(f'[epoch]: {epoch}, [Train Loss]: {t_loss.item()}, [Val Loss]: {test_loss.item()}')\n",
    "            if plateau>=epochs//3:\n",
    "                break\n",
    "#         liveloss.update(logs)\n",
    "#         liveloss.send()\n",
    "    return highest_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-06-04 14:48:07,712]\u001b[0m Finished trial#0 with value: 0.6719487309455872 with parameters: {'lr': 0.0001705750637645848, 'batch_size': 256, 'epochs': 197, 'layer_dims': 512, 'dropout': 0.16641997125852911}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 14:48:19,755]\u001b[0m Finished trial#1 with value: 0.22716806828975677 with parameters: {'lr': 0.002892827334811644, 'batch_size': 64, 'epochs': 10, 'layer_dims': 128, 'dropout': 0.20243223344441777}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 14:48:42,283]\u001b[0m Finished trial#2 with value: 0.24118924140930176 with parameters: {'lr': 0.0019385618422038513, 'batch_size': 512, 'epochs': 49, 'layer_dims': 256, 'dropout': 0.3241674866454065}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 14:51:46,857]\u001b[0m Finished trial#3 with value: 0.21350236237049103 with parameters: {'lr': 0.0786811962430303, 'batch_size': 256, 'epochs': 225, 'layer_dims': 256, 'dropout': 0.416387005309824}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 14:55:38,761]\u001b[0m Finished trial#4 with value: 0.20488935708999634 with parameters: {'lr': 0.0040797854461788715, 'batch_size': 64, 'epochs': 182, 'layer_dims': 64, 'dropout': 0.12404666168893422}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 14:59:33,090]\u001b[0m Finished trial#5 with value: 0.5414207577705383 with parameters: {'lr': 0.0023979184461447125, 'batch_size': 512, 'epochs': 195, 'layer_dims': 256, 'dropout': 0.21071712122583167}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:00:20,560]\u001b[0m Finished trial#6 with value: 0.2076503485441208 with parameters: {'lr': 0.009693462752977222, 'batch_size': 256, 'epochs': 73, 'layer_dims': 64, 'dropout': 0.13848290179532194}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:02:11,009]\u001b[0m Finished trial#7 with value: 0.520017683506012 with parameters: {'lr': 0.000980768818471568, 'batch_size': 256, 'epochs': 79, 'layer_dims': 32, 'dropout': 0.1155898187954629}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:08:38,806]\u001b[0m Finished trial#8 with value: 0.5568006038665771 with parameters: {'lr': 0.0009933582398952356, 'batch_size': 128, 'epochs': 214, 'layer_dims': 64, 'dropout': 0.1211721563252223}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:14:22,279]\u001b[0m Finished trial#9 with value: 0.5447965860366821 with parameters: {'lr': 0.0007320518851873856, 'batch_size': 256, 'epochs': 241, 'layer_dims': 128, 'dropout': 0.40370493322822076}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:19:59,020]\u001b[0m Finished trial#10 with value: 0.587276816368103 with parameters: {'lr': 2.1431799328170197e-05, 'batch_size': 128, 'epochs': 152, 'layer_dims': 512, 'dropout': 0.5540680309209757}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:25:05,937]\u001b[0m Finished trial#11 with value: 0.5840029716491699 with parameters: {'lr': 1.7541308294879805e-05, 'batch_size': 128, 'epochs': 144, 'layer_dims': 512, 'dropout': 0.5285775835590263}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:30:29,088]\u001b[0m Finished trial#12 with value: 0.5820685029029846 with parameters: {'lr': 2.412508961803926e-05, 'batch_size': 128, 'epochs': 146, 'layer_dims': 512, 'dropout': 0.5585853383248125}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:36:43,702]\u001b[0m Finished trial#13 with value: 0.6526041626930237 with parameters: {'lr': 0.00010372261489185057, 'batch_size': 128, 'epochs': 169, 'layer_dims': 512, 'dropout': 0.30750485720324106}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:43:13,462]\u001b[0m Finished trial#14 with value: 0.6492559909820557 with parameters: {'lr': 0.00011959653921703986, 'batch_size': 128, 'epochs': 175, 'layer_dims': 512, 'dropout': 0.28584670297540815}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:46:25,415]\u001b[0m Finished trial#15 with value: 0.6494693756103516 with parameters: {'lr': 0.00013456610191883898, 'batch_size': 256, 'epochs': 115, 'layer_dims': 512, 'dropout': 0.24936665631303334}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 15:49:10,546]\u001b[0m Finished trial#16 with value: 0.4724203944206238 with parameters: {'lr': 0.00014717519857232263, 'batch_size': 256, 'epochs': 111, 'layer_dims': 32, 'dropout': 0.3668048895409515}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:01:01,439]\u001b[0m Finished trial#17 with value: 0.6253289580345154 with parameters: {'lr': 6.152569299834567e-05, 'batch_size': 64, 'epochs': 205, 'layer_dims': 512, 'dropout': 0.4636564402699016}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:08:34,276]\u001b[0m Finished trial#18 with value: 0.6547470092773438 with parameters: {'lr': 0.0003313665043201995, 'batch_size': 128, 'epochs': 244, 'layer_dims': 512, 'dropout': 0.18818856701444298}. Best is trial#0 with value: 0.6719487309455872.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:13:28,209]\u001b[0m Finished trial#19 with value: 0.6728877425193787 with parameters: {'lr': 0.000382105863761183, 'batch_size': 512, 'epochs': 240, 'layer_dims': 512, 'dropout': 0.17588923979384496}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:17:25,219]\u001b[0m Finished trial#20 with value: 0.5267650485038757 with parameters: {'lr': 0.00033486319893682206, 'batch_size': 512, 'epochs': 250, 'layer_dims': 32, 'dropout': 0.17775202107516033}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:22:26,018]\u001b[0m Finished trial#21 with value: 0.6613136529922485 with parameters: {'lr': 0.00039102211604417653, 'batch_size': 512, 'epochs': 246, 'layer_dims': 512, 'dropout': 0.2536207130454826}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:27:06,400]\u001b[0m Finished trial#22 with value: 0.6544415354728699 with parameters: {'lr': 0.00038202771471109186, 'batch_size': 512, 'epochs': 229, 'layer_dims': 512, 'dropout': 0.25786320312622074}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:32:06,588]\u001b[0m Finished trial#23 with value: 0.6551649570465088 with parameters: {'lr': 4.4304212985482733e-05, 'batch_size': 512, 'epochs': 245, 'layer_dims': 512, 'dropout': 0.22914332066649717}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n",
      "\u001b[32m[I 2020-06-04 16:36:10,549]\u001b[0m Finished trial#24 with value: 0.6595775485038757 with parameters: {'lr': 0.00041665310836767617, 'batch_size': 512, 'epochs': 199, 'layer_dims': 512, 'dropout': 0.2504721677266377}. Best is trial#19 with value: 0.6728877425193787.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(train, n_trials=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "trial = study.best_trial\n",
    "trial.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df[target].value_counts().plot(kind=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x_train_stats = torch.from_numpy(X_train_stats.to_numpy()).long()\n",
    "x_train_text  = \n",
    "X_val_stats   = \n",
    "X_val_text    = \n",
    "y_train       = \n",
    "y_val         =  \n",
    "\n",
    "\n",
    "y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).long())\n",
    "\n",
    "x_val = torch.from_numpy(X_test.to_numpy()).long()\n",
    "y_val = torch.squeeze(torch.from_numpy(y_test.to_numpy()).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = './imp_sparse_features_train.csv'\n",
    "# target = 'Rating'\n",
    "# split = 0.3\n",
    "# x_train, x_val, y_train, y_val, tgt_type = read_preprocess.read_and_preprocess(file_path, target, split)\n",
    "x_train = torch.from_numpy(x_train.to_numpy()).float()\n",
    "y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).long())\n",
    "\n",
    "x_val = torch.from_numpy(x_val.to_numpy()).float()\n",
    "y_val = torch.squeeze(torch.from_numpy(y_val.to_numpy()).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_std)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,40,1)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer = PCA(n_components=35)\n",
    "pca = pca_transformer.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, batch_size, epochs, x_train, y_train, x_val, y_val,step_size, tgt_type):\n",
    "    print(y_train.shape[1])\n",
    "    highest_acc = 0\n",
    "    net = DNN(x_train.shape[1], tgt_type, [128,128,64,32],y_train.shape[1])\n",
    "    \n",
    "    if tgt_type==\"regression\":\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    params = {\n",
    "        'batch_size': 64,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "    training_set = dataset_csv(x_train, y_train)\n",
    "    train_loader = DataLoader(training_set, **params)\n",
    "    validation_set = dataset_csv(x_val, y_val)\n",
    "    val_loader = DataLoader(validation_set, **params)\n",
    "    \n",
    "    \n",
    "    net = net.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for a in optimizer.param_groups:\n",
    "            lr = cosine_lr(lr, epoch, step_size) + 1e-5\n",
    "            a['lr'] = lr\n",
    "        t_loss, a, acc = 0, 0, 0\n",
    "        log = {'y_train': [], 'y_val': [], 'y_pred': [], 'y_val_pred': []}\n",
    "        for x_train, y_train in train_loader:\n",
    "            a+=1\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            net.train()\n",
    "            y_pred = net(x_train)\n",
    "            train_loss = criterion(y_pred, torch.max(y_train, 1)[1])\n",
    "            t_loss += train_loss\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            acc += (y_pred.argmax(dim=1) == torch.max(y_train, 1)[1]).sum().float() / float(y_train.size(0))\n",
    "        print(acc/a)\n",
    "        t_loss /= a\n",
    "        if epoch%1 == 0:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                test_loss, a, acc = 0, 0, 0\n",
    "                for x_val, y_val in val_loader:\n",
    "                    a += 1\n",
    "                    x_val, y_val = x_val.to(device),y_val.to(device)\n",
    "                    y_test_pred = net(x_val)\n",
    "                    y_test_pred = torch.squeeze(y_test_pred)\n",
    "                    test_loss += criterion(y_test_pred, torch.max(y_val, 1)[1])\n",
    "                    acc += (y_test_pred.argmax(dim=1) == torch.max(y_val, 1)[1]).sum().float() / float(y_val.size(0))\n",
    "            if (acc/a)>highest_acc: \n",
    "                highest_acc = acc/a\n",
    "                torch.save(net.state_dict(),\"./model_info/14_may_1.pth\")\n",
    "            print(acc/a)\n",
    "            test_loss /= a\n",
    "            print(f'[epoch]: {epoch}, [Train Loss]: {t_loss.item()}, [Val Loss]: {test_loss.item()}')\n",
    "        \n",
    "        if tgt_type==\"regression\":\n",
    "            if epoch%30==0:\n",
    "                scaler_t = pickle.load(open('./model_info/scaler_t.pkl','rb'))\n",
    "                y_0 = scaler_t.inverse_transform(log['y_train'])\n",
    "                y_1 = scaler_t.inverse_transform(log['y_pred'])\n",
    "                rmse = np.sqrt(mean_squared_error(y_0,y_1))\n",
    "                print(rmse)\n",
    "                y_0 = scaler_t.inverse_transform(log['y_val'])\n",
    "                y_1 = scaler_t.inverse_transform(log['y_val_pred'])\n",
    "                rmse = np.sqrt(mean_squared_error(y_0,y_1))\n",
    "                print(rmse)\n",
    "    return net, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, log = train(0.003, 512, 100, x_train, y_train, x_val, y_val, 10, tgt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = './upvotes/train.csv'\n",
    "# target = 'Upvotes'\n",
    "\n",
    "# X,y = read_and_preprocess_test(file_path, target, tgt_type)\n",
    "ckpt_dir = \"./model_info/14_may_1.pth\"\n",
    "def test(X,y, tgt_type, ckpt_dir):\n",
    "    trained_model = DNN(x_train.shape[1], tgt_type, [128,128,64,32],y_train.shape[1])\n",
    "    trained_model.load_state_dict(torch.load(ckpt_dir))\n",
    "    trained_model\n",
    "    x_test = X\n",
    "    if y is not None:\n",
    "        y_test = y\n",
    "    trained_model.eval()\n",
    "    y_pred = trained_model(x_test)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(x_val,y_val,tgt_type,ckpt_dir)\n",
    "enc = pickle.load(open('./model_info/OHE.pkl','rb'))\n",
    "df = pd.DataFrame(columns=['gt','predictions'])\n",
    "# df[['predictions']] = scaler_t.inverse_transform(df[['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt = torch.max(y_val, 1)[1].detach().numpy()\n",
    "df = pd.DataFrame(gt)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['gt','predictions'])\n",
    "df['gt'] = gt\n",
    "df['predictions'] = predictions.argmax(dim=1)\n",
    "pred_df = pd.DataFrame(classification_report(df['gt'], df['predictions'], output_dict=True))\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(df['gt'], df['predictions']))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './CICIDS2017/combined_data_test.csv'\n",
    "target = 'Label'\n",
    "x_test, y_test = read_preprocess.read_and_preprocess_test(file_path, target, split)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.to_numpy()).float()\n",
    "y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(x_test,y_test,tgt_type,ckpt_dir)\n",
    "enc = pickle.load(open('./model_info/OHE.pkl','rb'))\n",
    "df = pd.DataFrame(columns=['gt','predictions'])\n",
    "gt = torch.max(y_test, 1)[1].detach().numpy()\n",
    "df = pd.DataFrame(gt)\n",
    "df = pd.DataFrame(columns=['gt','predictions'])\n",
    "df['gt'] = gt\n",
    "df['predictions'] = predictions.argmax(dim=1)\n",
    "pred_df = pd.DataFrame(classification_report(df['gt'], df['predictions'], output_dict=True))\n",
    "pred_df, pd.DataFrame(confusion_matrix(df['gt'], df['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((gt.size, gt.max()+1))\n",
    "b[np.arange(gt.size),gt] = 1\n",
    "gt2 = b\n",
    "\n",
    "pred = predictions.argmax(dim=1).numpy()\n",
    "b = np.zeros((pred.size,pred.max()+1))\n",
    "b[np.arange(pred.size),pred] = 1\n",
    "pred = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.inverse_transform(gt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gt'] = enc.inverse_transform(gt2)\n",
    "df['predictions'] = enc.inverse_transform(pred)\n",
    "pred_df = pd.DataFrame(classification_report(df['gt'], df['predictions'], output_dict=True))\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['gt'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(df['gt'], df['predictions'],normalize = 'all'), index = labels,columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
